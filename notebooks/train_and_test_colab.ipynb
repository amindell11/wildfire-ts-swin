{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SwinUnet Wildfire Training & Test (Colab)\n",
        "\n",
        "This notebook trains SwinUnet on a single fold of the WildfireSpreadTS HDF5 dataset, then evaluates on the held-out test split and reports Average Precision (AP) and F1.\n",
        "\n",
        "**Prerequisites:**\n",
        "- HDF5 dataset already on Google Drive (from `download_and_convert_dataset.ipynb`)\n",
        "- **Runtime → Change runtime type → GPU** (T4 or better recommended)\n",
        "- A [GitHub Personal Access Token](https://github.com/settings/tokens) stored as a Colab secret named `GITHUB_TOKEN` (Colab sidebar → key icon → Add new secret)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration (user-editable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPO_ORG   = \"amindell11\"   # Replace with your GitHub username or organisation\n",
        "REPO_NAME  = \"wildfire-ts-swin\"\n",
        "HDF5_DIR   = \"/content/drive/MyDrive/wildfire_dataset/hdf5\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/wildfire_runs/fold0\"\n",
        "\n",
        "DATA_FOLD_ID             = 0      # 0–11; which train/val/test year split\n",
        "N_LEADING_OBSERVATIONS   = 1      # 1 or 5\n",
        "MAX_EPOCHS               = 100\n",
        "BATCH_SIZE               = 16\n",
        "BASE_LR                  = 1e-4\n",
        "FOCAL_GAMMA              = 2.0\n",
        "CROP_SIDE_LENGTH         = 128\n",
        "SEED                     = 42\n",
        "NUM_WORKERS              = 2      # keep low on Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Clone repo and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into '/content/wildfire-ts-swin'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 64 (delta 14), reused 64 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (64/64), 51.91 KiB | 1.79 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "_repo_url = f\"https://github.com/amindell11/wildfire-ts-swin.git\"\n",
        "!rm -rf /content/wildfire-ts-swin\n",
        "!git clone $_repo_url /content/wildfire-ts-swin\n",
        "!pip install -q -r /content/wildfire-ts-swin/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, f'/content/{REPO_NAME}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Verify GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: No CUDA device detected. Training will be slow or fail.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: No CUDA device detected. Training will be slow or fail.\")\n",
        "else:\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets.wildfire'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2230504924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSwinUnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrainer_wildfire\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer_wildfire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwildfire\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mN_FEATURES_PER_TIMESTEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/wildfire-ts-swin/trainer_wildfire.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwildfire\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWildfireDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_year_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFocalLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_binary_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_ap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets.wildfire'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import types\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from config import get_config\n",
        "from networks.vision_transformer import SwinUnet\n",
        "from trainer_wildfire import trainer_wildfire\n",
        "from datasets.wildfire import N_FEATURES_PER_TIMESTEP\n",
        "\n",
        "cudnn.benchmark = True\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "in_chans = N_LEADING_OBSERVATIONS * N_FEATURES_PER_TIMESTEP\n",
        "extra_opts = [\n",
        "    'MODEL.SWIN.IN_CHANS', str(in_chans),\n",
        "    'MODEL.PRETRAIN_CKPT', 'None',\n",
        "]\n",
        "\n",
        "args = types.SimpleNamespace(\n",
        "    data_dir=HDF5_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    n_leading_observations=N_LEADING_OBSERVATIONS,\n",
        "    n_leading_observations_test_adjustment=N_LEADING_OBSERVATIONS,\n",
        "    crop_side_length=CROP_SIDE_LENGTH,\n",
        "    load_from_hdf5=True,\n",
        "    data_fold_id=DATA_FOLD_ID,\n",
        "    max_epochs=MAX_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    base_lr=BASE_LR,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    eval_interval=1,\n",
        "    seed=SEED,\n",
        "    n_gpu=1,\n",
        "    focal_gamma=FOCAL_GAMMA,\n",
        "    cfg=f'/content/{REPO_NAME}/configs/swin_tiny_patch4_window4_128_wildfire.yaml',\n",
        "    opts=extra_opts,\n",
        "    zip=False,\n",
        "    cache_mode='part',\n",
        "    resume=None,\n",
        "    accumulation_steps=None,\n",
        "    use_checkpoint=False,\n",
        "    amp_opt_level='O1',\n",
        "    tag=None,\n",
        "    eval=False,\n",
        "    throughput=False,\n",
        ")\n",
        "\n",
        "config = get_config(args)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "net = SwinUnet(config, img_size=config.DATA.IMG_SIZE, num_classes=2).cuda()\n",
        "print(f\"Model in_chans={in_chans}  (n_leading_observations={N_LEADING_OBSERVATIONS} × 40 features)\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in net.parameters()) / 1e6:.1f}M\")\n",
        "\n",
        "trainer_wildfire(args, net, OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate on test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from datasets.wildfire import WildfireDataset, get_year_split\n",
        "from utils import compute_binary_metrics, compute_ap\n",
        "\n",
        "train_years, val_years, test_years = get_year_split(DATA_FOLD_ID)\n",
        "\n",
        "ckpt_path = f\"{OUTPUT_DIR}/best_model.pth\"\n",
        "state_dict = torch.load(ckpt_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if list(state_dict.keys())[0].startswith('module.'):\n",
        "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "net.load_state_dict(state_dict)\n",
        "net.eval()\n",
        "\n",
        "db_test = WildfireDataset(\n",
        "    data_dir=HDF5_DIR,\n",
        "    included_fire_years=test_years,\n",
        "    is_train=False,\n",
        "    stats_years=train_years,\n",
        "    n_leading_observations=N_LEADING_OBSERVATIONS,\n",
        "    n_leading_observations_test_adjustment=N_LEADING_OBSERVATIONS,\n",
        "    crop_side_length=CROP_SIDE_LENGTH,\n",
        "    load_from_hdf5=True,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    db_test, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True,\n",
        ")\n",
        "\n",
        "all_probs, all_preds, all_gts = [], [], []\n",
        "with torch.no_grad():\n",
        "    for x_batch, y_batch in tqdm(test_loader, desc=\"Test\"):\n",
        "        x_batch = x_batch.cuda()\n",
        "        logits = net(x_batch)\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(np.int64)\n",
        "        gts = y_batch.numpy()\n",
        "        all_probs.append(probs.flatten())\n",
        "        all_preds.append(preds.flatten())\n",
        "        all_gts.append(gts.flatten())\n",
        "\n",
        "all_probs = np.concatenate(all_probs)\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_gts = np.concatenate(all_gts)\n",
        "\n",
        "metrics = compute_binary_metrics(all_preds, all_gts)\n",
        "ap = compute_ap(all_probs, all_gts)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"Test Results\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Test AP   : {ap:.4f}\")\n",
        "print(f\"Test F1   : {metrics['f1']:.4f}\")\n",
        "print(f\"Precision : {metrics['precision']:.4f}\")\n",
        "print(f\"Recall    : {metrics['recall']:.4f}\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Note on full 12-fold evaluation\n",
        "\n",
        "To reproduce the paper's mean ± std AP, run this notebook **12 times** with `DATA_FOLD_ID` set to 0, 1, 2, … 11 (or run a loop in a script). Each fold uses a different train/val/test year split. Average the 12 test AP values to get the reported metric."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
